[{"content":"GitHub Repository A full example of this code can be found at https://github.com/dombarter/Solar.API\nMotivation As you may have seen from my previous posts, I\u0026rsquo;ve been experimenting with creating a .NET 6 Web API that uses Entity Framework, Identity and JWTs as cookies that can be consumed by a Vue.js SPA. I\u0026rsquo;ve not delved into the world of integration tests too much but thought it would be a really interesting concept to create an entire test database and test our full controller lifecycle, as this will catch errors at any level.\nCreate Your Test Project The first step is to make a xUnit test project to complement your API project.\nOur API project is called Solar.API and we created a new xUnit project called Solar.API.Test.\nNuGet Packages You need to install the following NuGet packages:\n Microsoft.AspNetCore.Mvc.Testing - for mocking the controllers Microsoft.EntityFrameworkCore.InMemory - for creating an empty test database  API Startup Class Our integration tests need to access our startup class, Program.cs so they can create a mock of the controllers. We need to make some slight adjustments to the access levels because by default it is only internally accessible.\nGo to Program.cs and add this line at the bottom of the file:\napp.Run(); public partial class Program {} // \u0026lt;-- add this line! Integration Tests Base Class Now we need to make a base class that all our integration tests will inherit. In this class we will provide a few important things:\n GenerateClient - for creating a mock of the controllers, and swapping out our SQL Server for an empty in-memory equivalent Authentication helper methods  GenerateClient The GenerateClient performs the following:\n Creates a new WebApplicationFactory based on our API startup class It then amends the startup class, by changing some of the services It firstly removes the existing DbContext service It then replaces it with a new DbContext class that uses an in-memory database with a unique name (this ensures each integration test has an empty database to work with) It finally defines a custom https endpoint for the mocked controller, meaning our Secure JWT cookies can be transmitted  public class IntegrationTestsBase { protected HttpClient GenerateClient() { // Build up the API, using an in memory database  var application = new WebApplicationFactory\u0026lt;Program\u0026gt;() .WithWebHostBuilder(builder =\u0026gt; { builder.ConfigureServices(services =\u0026gt; { // Remove the db context  services.RemoveAll(typeof(DbContextOptions\u0026lt;SolarDbContext\u0026gt;)); // Replace the connection with an in memory equivalent  var dbName = Guid.NewGuid().ToString(); services.AddDbContext\u0026lt;SolarDbContext\u0026gt;(options =\u0026gt; { options.UseInMemoryDatabase(dbName); }); }); }); // Generate the client  var client = application.CreateClient(new WebApplicationFactoryClientOptions { BaseAddress = new Uri(\u0026#34;https://localhost\u0026#34;) }); return client; } } Write Some Tests! You can now write some tests. Here are some examples of what you can do:\nMaking Sure Endpoints Are Protected in this example we make GET requests to multiple endpoints we know should be protected under normal circumstances. We make sure that a 401 code is returned.\npublic class MoonControllerTests : IntegrationTestsBase { [Theory] [InlineData(\u0026#34;/moons/one\u0026#34;)] [InlineData(\u0026#34;/moons/two\u0026#34;)] public async Task Get_WhenUnauthenticated_ReturnsUnauthorized(string url) { // Arrange  var client = GenerateClient(); // Act  var response = await client.GetAsync(url); // Assert  Assert.Equal(HttpStatusCode.Unauthorized, response.StatusCode); } } Registering \u0026amp; Logging In In this example we register a new user, then login as them and make sure that both requests return an OK response indicating the request was a success.\npublic class AccountControllerTests : IntegrationTestsBase { [Fact] public async Task Post_Login_ReturnsOK() { // Arrange  var client = GenerateClient(); // Act  var register = await Register(client, new RegisterDto { Email = \u0026#34;dom@email.com\u0026#34;, Password = \u0026#34;password\u0026#34; }); var login = await Login(client, new LoginDto { Email = \u0026#34;dom@email.com\u0026#34;, Password = \u0026#34;password\u0026#34; }); // Assert  Assert.Equal(HttpStatusCode.OK, register.StatusCode); Assert.Equal(HttpStatusCode.OK, login.StatusCode); } } Where Login and Register are some helper methods I was talking about earlier:\nprotected async Task\u0026lt;HttpResponseMessage\u0026gt; Register(HttpClient client, RegisterDto registerDto) { // Act  var response = await client.PostAsJsonAsync(\u0026#34;/user/register\u0026#34;, registerDto); return response; } protected async Task\u0026lt;HttpResponseMessage\u0026gt; Login(HttpClient client, LoginDto loginDto) { // Act  var response = await client.PostAsJsonAsync(\u0026#34;/user/login\u0026#34;, loginDto); return response; } Requesting An Endpoint When Authenticated In this example, we authenticate the request by registering and logging in using a helper method. This stores a JWT cookie. We then make a request to an endpoint that would usually be protected and make sure that the response is OK.\npublic class MoonControllerTests : IntegrationTestsBase { [Fact] public async Task Get_OneMoon_AsUser() { // Arrange  var client = GenerateClient(); await AuthenticateAsUser(client); // Act  var response = await client.GetAsync(\u0026#34;/moons/one\u0026#34;); // Assert  Assert.Equal(HttpStatusCode.OK, response.StatusCode); } } References Based on the following websites:\n https://docs.microsoft.com/en-us/aspnet/core/test/integration-tests?view=aspnetcore-6.0 https://stackoverflow.com/questions/70900451/c-sharp-netcore-webapi-integration-testing-httpclient-uses-https-for-get-reques  ","permalink":"https://dombarter.co.uk/posts/add-integration-tests-for-dotnet-web-api/","summary":"Test the full lifecycle of your API controllers using integration tests. Combined with in-memory databases you can fully mock your system and reproducible results. This approach even deals with cookies, so you can authenticate using a JWT in a cookie.","title":"Add integration tests to an existing .NET 6 Web API using Entity Framework, Identity and JWTs"},{"content":"Motivation I have a Vue.js SPA that communicates with a .NET 6 Web API for the majority of things. It uses JWTs in cookies for authentication and authorisation. The API generates the JWTs. However, I would like some Azure Functions (microservices) to perform scalable operations as to not stress the API. These Azure Functions will need authenticating otherwise anyone can call them.\nLet\u0026rsquo;s authenticate them using the JWT cookie our API has given us.\nNuGet Packages Install the following NuGet package.\n System.IdentityModel.Tokens.Jwt (\u0026lt;= 6.10.2)  The package version must be lower than 6.11 due to https://github.com/Azure/azure-functions-host/issues/7878.\nApp Settings We need to set some details about our JWTs in our local.settings.json so they can be validated. These should match the details used by the API where the JWT is generated.\n{ \u0026#34;Values\u0026#34;: { \u0026#34;JwtKey\u0026#34;: \u0026#34;ThisIsMySecretKey\u0026#34;, \u0026#34;JwtIssuer\u0026#34;: \u0026#34;https://localhost:7234/\u0026#34;, \u0026#34;JwtAudience\u0026#34;: \u0026#34;https://localhost:7234/\u0026#34;, \u0026#34;JwtCookieName\u0026#34;: \u0026#34;solar-access-token\u0026#34; } } Token Validator Now we need to make a method (could be static, or passed via dependency injection), that will validate the request:\npublic static bool TokenIsValid(HttpRequest req) { try { string cookie = req.Cookies[Environment.GetEnvironmentVariable(\u0026#34;JwtCookieName\u0026#34;)] ?? \u0026#34;\u0026#34;; var tokenParams = new TokenValidationParameters() { ValidateIssuer = true, ValidateAudience = true, ValidateLifetime = true, ValidateIssuerSigningKey = true, ValidIssuer = Environment.GetEnvironmentVariable(\u0026#34;JwtIssuer\u0026#34;), ValidAudience = Environment.GetEnvironmentVariable(\u0026#34;JwtAudience\u0026#34;), IssuerSigningKey = new SymmetricSecurityKey(Encoding.UTF8.GetBytes(Environment.GetEnvironmentVariable(\u0026#34;JwtKey\u0026#34;))) }; new JwtSecurityTokenHandler().ValidateToken(cookie, tokenParams, out var securityToken); return true; } catch { return false; } } Token Validation We can now call this method in each Function, and the JWT within the cookie will be validated:\npublic static async Task\u0026lt;IActionResult\u0026gt; Run([HttpTrigger(AuthorizationLevel.Anonymous, \u0026#34;get\u0026#34;, \u0026#34;post\u0026#34;, Route = null)] HttpRequest req) { // Validate the access token  if (!TokenValidator.TokenIsValid(req)) { return new UnauthorizedResult(); } } References  https://docs.microsoft.com/en-us/dotnet/api/system.identitymodel.tokens.jwt.jwtsecuritytokenhandler.validatetoken?view=azure-dotnet  ","permalink":"https://dombarter.co.uk/posts/validate-jwt-cookies-in-dotnet-functions/","summary":"Validate JWTs as cookies, that are sent to an Azure Functions application","title":"Validate JWT cookies sent to an Azure Functions application"},{"content":"Vue.js comes with a built in way to inject different variables depending on what type of build you perform.\nBy default when you run npm run serve, NODE_ENV is set to development. When you run npm run build, NODE_ENV is to set to production.\nIf you reference an environment variable beginning with the prefix VUE_APP_, (eg. process.env.VUE_APP_MY_VARIABLE), this will be replaced with the actual value at compile time by webpack.\nSetting Environment Variables You can obviously set environment variables on your machine directly, but this is hard to maintain. Instead you can utilise dotenv which is built into webpack.\n Environment variables set into .env.development will be loaded when you run npm run serve. Environment variables set into .env.production will be loaded when you run npm run build.  There are also other ways of setting environment variables, such as in GitHub workflows, removing the need for the .env.production file.\nExample Let\u0026rsquo;s imagine that our single page application has to consume an API. When we\u0026rsquo;re running locally this will reference a localhost address, but then a different address once deployed.\n.env.development\nVUE_APP_API_BASE_URL=http://localhost:3000 .env.production\nVUE_APP_API_BASE_URL=https://my-api.com Then in our code, we can reference the following, at it will be replaced at compile time:\nconsole.log(process.env.VUE_APP_API_BASE_URL); References  https://cli.vuejs.org/guide/mode-and-env.html#environment-variables  ","permalink":"https://dombarter.co.uk/posts/configure-vuejs-with-environment-variables/","summary":"Inject environment specific settings into your Vue.js build, allowing you to change key configuration such as API endpoints","title":"Configure Vue.js with environment variables"},{"content":"GitHub Repository An example of this code can be found at https://github.com/dombarter/Solar.API\nMotivation Ever wondered if there was an easy way of turning your C# DTOs into TypeScript interfaces? Every wondered if we could automatically generate a TypeScript client for our API, rather than having to manually write out the function for every endpoint? Turns out the answer is yes, here\u0026rsquo;s how:\nGenerate The Swagger Definition If you\u0026rsquo;ve recently created a .NET Web API you\u0026rsquo;ll know it automatically opens up Swagger when you run the application. However it doesn\u0026rsquo;t create a swagger.json file that is copied to your project folder.\nLet\u0026rsquo;s get a swagger.json being generated:\nInstall Swashbuckle CLI Run the following commands in your API project:\ndotnet new tool-manifest dotnet tool install Swashbuckle.AspNetCore.Cli Add A New Post-Build Command Add the following task to your API csproj which will generate a swagger.json file from our API build, and then copy that file into our Vue.js directory, so we can access it using npm commands.\n\u0026lt;Target Name=\u0026#34;OpenAPI\u0026#34; AfterTargets=\u0026#34;Build\u0026#34;\u0026gt; \u0026lt;Exec Command=\u0026#34;dotnet tool restore\u0026#34; WorkingDirectory=\u0026#34;$(ProjectDir)\u0026#34; /\u0026gt; \u0026lt;Exec Command=\u0026#34;dotnet swagger tofile --output ../Solar.Vue/references/swagger.json $(OutputPath)$(AssemblyName).dll v1\u0026#34; WorkingDirectory=\u0026#34;$(ProjectDir)\u0026#34; /\u0026gt; \u0026lt;/Target\u0026gt; If you now build the project, you should find a swagger.json file under Solar.Vue/references/.\nGenerate The TypeScript Client Now we have our hands on a swagger.json we now need to build a TypeScript client. There are a couple of steps:\nInstall OpenAPI Tools Install the following npm package:\nnpm i @openapitools/openapi-generator-cli -D OpenAPI Tools is the officially supported package for everything OpenAPI/Swagger (see https://openapi.tools/)\nInstall Java The above generator requires Java to be installed. If you\u0026rsquo;re on windows you can easily install it using chocolatey:\nchoco install oraclejdk NPM Script And finally, we need to add a npm script to build the client:\n\u0026#34;generate-api-client\u0026#34;: \u0026#34;openapi-generator-cli generate -i ./references/swagger.json -g typescript-axios -o ./src/api/\u0026#34;  ./references/swagger.json defines where the definition is ./src/api defines where the client should be created typescript-axios is a type of client (specifically written in TypeScript and using axios for http calls)  Testing You will now have a fully fledged TypeScript client with DTO types. Here is an example of how you can use it!\nimport { AccountApi } from \u0026#34;@/api\u0026#34;; const api = new AccountApi(); const result = await api.userLoginPost({ email: \u0026#34;user@email.com\u0026#34;, password: \u0026#34;password\u0026#34;, }); References  https://www.npmjs.com/package/@openapitools/openapi-generator-cli https://khalidabuhakmeh.com/generate-aspnet-core-openapi-spec-at-build-time https://chrlschn.medium.com/net-6-web-apis-with-openapi-typescript-client-generation-a743e7f8e4f5 https://stackoverflow.com/questions/33283071/swagger-webapi-create-json-on-build  ","permalink":"https://dombarter.co.uk/posts/generate-typescript-client-from-swagger/","summary":"Turn your swagger.json file into a fully configured typescript API service, that you can instantly integrate into your JavaScript application","title":"Generate a .NET 6 Web API Typescript client from an OpenAPI / Swagger definition"},{"content":"GitHub Repository All the code in this series can be found at https://github.com/dombarter/Solar.API\nMotivation In my previous post, we setup a full .NET 6 Web API with user and role management with .NET Identity, and then we\u0026rsquo;ve authorised and authenticated those users using JWTs.\nHowever, there are security risks surrounding returning the JWT as plain text and letting the client handle the storage of it. This is because the most likely place it would be stored is in localStorage which is highly susceptible to XSS (cross-site scripting). If you were consuming this application using Postman or a mobile application this wouldn\u0026rsquo;t be a problem. But for an SPA (single page application) it is.\nInstead, we are going to change the way the JWT is returned, so that it is returned as an expirable, http only cookie; which cannot be accessed by javascript and will be automatically included on every request.\nCookie Name We first of all need to define a name for our access token cookie so we\u0026rsquo;re always reading and writing to the same one. This needs to be added in appsettings.json under the Jwt section:\n{ \u0026#34;Jwt\u0026#34;: { \u0026#34;Key\u0026#34;: \u0026#34;ThisIsMySecretKey\u0026#34;, \u0026#34;Issuer\u0026#34;: \u0026#34;https://localhost:7234/\u0026#34;, \u0026#34;Audience\u0026#34;: \u0026#34;https://localhost:7234/\u0026#34;, \u0026#34;CookieName\u0026#34;: \u0026#34;solar-access-token\u0026#34; // \u0026lt;- add this line } } Alter The Authentication Mechanism Now we need to go to the startup class, Program.cs and add an extra option to the AddJwtBearer method which will pull the token from a cookie rather than from its default location in the Authorization header:\noptions.Events = new JwtBearerEvents { OnMessageReceived = context =\u0026gt; { context.Token = context.Request.Cookies[builder.Configuration[\u0026#34;Jwt:CookieName\u0026#34;]]; return Task.CompletedTask; }, }; Generate The Cookie On Login We\u0026rsquo;re now reading the JWT from the cookie, but we need to generate the cookie and send it to the client as well. Amend your login action so it appends a cookie to the response and only returns account information in the body (no access tokens!):\nResponse.Cookies.Append(_config[\u0026#34;Jwt:CookieName\u0026#34;], token, new CookieOptions { HttpOnly = true, IsEssential = true, MaxAge = TimeSpan.FromMinutes(30), SameSite = SameSiteMode.None, Secure = true, }); return new OkObjectResult( new LoginResultDto(user.Email, await _userManager.GetRolesAsync(user)) ); Clear The Cookie On Logout Because the client cannot access the cookie at all, the only way we can clear it is by calling a logout action, that will return an empty cookie with an instant expiry:\n[HttpPost] [Route(\u0026#34;logout\u0026#34;)] [AllowAnonymous] public IActionResult Logout() { Response.Cookies.Append(_config[\u0026#34;Jwt:CookieName\u0026#34;], string.Empty, new CookieOptions { HttpOnly = true, IsEssential = true, MaxAge = TimeSpan.Zero, SameSite = SameSiteMode.None, Secure = true, }); return Ok(); } Update The Swagger Configuration Now that we no longer need to manually include the JWT with each request, we can remove the Swagger configuration that gave us that Authorization button at the top right:\nbuilder.Services.AddEndpointsApiExplorer(); builder.Services.AddSwaggerGen(options =\u0026gt; { var jwtSecurityScheme = new OpenApiSecurityScheme { Name = \u0026#34;Authorization\u0026#34;, Type = SecuritySchemeType.Http, Scheme = JwtBearerDefaults.AuthenticationScheme, BearerFormat = \u0026#34;JWT\u0026#34;, In = ParameterLocation.Header, Reference = new OpenApiReference { Type = ReferenceType.SecurityScheme, Id = JwtBearerDefaults.AuthenticationScheme } }; options.AddSecurityDefinition(JwtBearerDefaults.AuthenticationScheme, jwtSecurityScheme); options.AddSecurityRequirement(new OpenApiSecurityRequirement(){{ jwtSecurityScheme, new string[] {} }}); }); becomes\nbuilder.Services.AddEndpointsApiExplorer(); builder.Services.AddSwaggerGen(); Testing You should now be able to open the Swagger page for your API, make a login request and then make a call to a protected endpoint and you will get a successful response. If you look in the dev tools you should notice the solar-access-token has been set. Try removing it and see what happens!\nConnecting To A Vue.js SPA Cookies work when using a system such as Swagger, but when using them in Vue.js we need a bit more configuration. This is because it is another web site, so CORs comes into play.\nFirst of all we need to add a default CORs policy to our API. Make sure to define specific origins rather than allowing all. This is applied in Program.cs:\nbuilder.Services.AddCors(options =\u0026gt; options.AddDefaultPolicy( policy =\u0026gt; policy .WithOrigins(\u0026#34;http://localhost:8080\u0026#34;) .AllowCredentials() .AllowAnyHeader() .AllowAnyMethod() )); ... app.UseCors(); And then finally, if you are using a http client such as axios - make sure to set withCredentials to true to make sure those cookies are always sent and received:\nconst axiosInstance = axios.create({ withCredentials: true, }); Conclusion And that\u0026rsquo;s it, we now have a way of securely managing users, roles and access to our API when calling from a JavaScript SPA.\nReferences  https://javascript.plainenglish.io/how-to-secure-jwt-in-a-single-page-application-6a46e69fc393 https://spin.atomicobject.com/2020/07/25/net-core-jwt-cookie-authentication/ https://dotnetcoretutorials.com/2017/01/15/httponly-cookies-asp-net-core/ https://docs.microsoft.com/en-us/dotnet/api/microsoft.aspnetcore.http.cookieoptions?view=aspnetcore-6.0#properties https://blog.logrocket.com/jwt-authentication-best-practices/ https://stackoverflow.com/questions/71419379/set-cookie-not-working-properly-in-axios-call  ","permalink":"https://dombarter.co.uk/posts/add-dotnet-jwts-for-spas-to-web-api/","summary":"Change how we handle JWTs so that they can be securely used by SPAs rather than the standard API lifecycle","title":"(Part 4) Amend JWTs to work securely with SPAs in a .NET 6 Web API"},{"content":"GitHub Repository All the code in this series can be found at https://github.com/dombarter/Solar.API\nMotivation In my previous post, I explored how to manage login, registration and role assignment of our users using .NET Identity. In this post I am going to add JWT based authentication and authorization to our .NET 6 Web API.\nWhat are JWTs?  JSON Web Token (JWT) is an open standard (RFC 7519) that defines a compact and self-contained way for securely transmitting information between parties as a JSON object. This information can be verified and trusted because it is digitally signed.\n (https://jwt.io/introduction)\nJWTs are becoming increasingly popular in the world of single sign on, and are a very scalable, stateless and memory efficient way of providing authentication.\nNuGet Packages We need to install the following package:\n Microsoft.AspNetCore.Authentication.JwtBearer  Define The JWT Configuration JWTs are defined by some key parts including:\n Issuer (who has issued the key) Audience (who the key is intended for) Signing Key (a private random string used to sign the key)  We need to define these in our appsettings.json:\n{ \u0026#34;Jwt\u0026#34;: { \u0026#34;Key\u0026#34;: \u0026#34;ThisIsMySecretKey\u0026#34;, \u0026#34;Issuer\u0026#34;: \u0026#34;localhost\u0026#34;, \u0026#34;Audience\u0026#34;: \u0026#34;localhost\u0026#34; } } Don\u0026rsquo;t worry too much about the value of the Issuer and Audience, just make sure they\u0026rsquo;re the same. When used in a single sign on setting - we might have one machine generate the key, which could then be consumed by multiple different places. In our case we\u0026rsquo;re creating and consuming it in the same place. It\u0026rsquo;s just a straight string comparison.\nFor more information see https://www.rfc-editor.org/rfc/rfc7519#section-4.1.\nAmend The Startup Now we need to alter our startup class, Program.cs to configure our authentication mechanism. Specifically we are telling the Web API that it should look out for a JWT in the request headers, and use this to authenticate the current user.\n// Add JWTs builder.Services.AddAuthentication(auth =\u0026gt; { auth.DefaultAuthenticateScheme = JwtBearerDefaults.AuthenticationScheme; auth.DefaultChallengeScheme = JwtBearerDefaults.AuthenticationScheme; }).AddJwtBearer(options =\u0026gt; { options.SaveToken = true; options.TokenValidationParameters = new TokenValidationParameters { ValidateIssuer = true, ValidateAudience = true, ValidateLifetime = true, ValidateIssuerSigningKey = true, ValidIssuer = builder.Configuration[\u0026#34;Jwt:Issuer\u0026#34;], ValidAudience = builder.Configuration[\u0026#34;Jwt:Audience\u0026#34;], IssuerSigningKey = new SymmetricSecurityKey(Encoding.UTF8.GetBytes(builder.Configuration[\u0026#34;Jwt:Key\u0026#34;])) }; }); ... // Must be in this order app.UseAuthentication(); app.UseAuthorization(); Token Service We now need to make a token service, that will accept an IdentityUser and an expiry time, before generating a JWT we can return to the user.\n// TokenService.cs  using Microsoft.AspNetCore.Identity; using Microsoft.Extensions.Configuration; using Microsoft.IdentityModel.Tokens; using System.IdentityModel.Tokens.Jwt; using System.Security.Claims; using System.Text; namespace Solar.Services.Token { public class TokenService : ITokenService { private readonly UserManager\u0026lt;IdentityUser\u0026gt; _userManager; private readonly IConfiguration _config; public TokenService(UserManager\u0026lt;IdentityUser\u0026gt; userManager, IConfiguration config) { _userManager = userManager; _config = config; } async public Task\u0026lt;string\u0026gt; GenerateJwtToken(IdentityUser user, TimeSpan expiration) { // Define the token claims (username and unique guid)  var claims = new List\u0026lt;Claim\u0026gt; { new Claim(JwtRegisteredClaimNames.Sub, user.UserName), new Claim(JwtRegisteredClaimNames.Jti, Guid.NewGuid().ToString()), }; // Add the roles to the token  foreach(var role in await _userManager.GetRolesAsync(user)) { claims.Add(new Claim(\u0026#34;role\u0026#34;, role)); } // Encode our private JWT key  var key = new SymmetricSecurityKey(Encoding.UTF8.GetBytes(_config[\u0026#34;Jwt:Key\u0026#34;])); var creds = new SigningCredentials(key, SecurityAlgorithms.HmacSha256); // Put everything together  var token = new JwtSecurityToken( issuer: _config[\u0026#34;Jwt:Issuer\u0026#34;], audience: _config[\u0026#34;Jwt:Audience\u0026#34;], expires: DateTime.UtcNow.Add(expiration), claims: claims, signingCredentials: creds ); // Build the token as a string  return new JwtSecurityTokenHandler().WriteToken(token); } } } Consuming The Token Service Now we have the token service setup we need to register it in the dependency injection container within Program.cs:\n// Add token service builder.Services.AddTransient\u0026lt;ITokenService, TokenService\u0026gt;(); We can then utilise the token service in our login action like so:\n[HttpPost] [Route(\u0026#34;login\u0026#34;)] [AllowAnonymous] public async Task\u0026lt;ActionResult\u0026lt;string\u0026gt;\u0026gt; Login([FromBody] LoginDto model) { var result = await _signInManager.PasswordSignInAsync(model.Email, model.Password, model.RememberMe, false); if (!result.Succeeded) { return BadRequest(\u0026#34;Incorrect email or password\u0026#34;); } // Generate JWT  var user = await _userManager.FindByNameAsync(model.Email); var token = await _tokenService.GenerateJwtToken(user, TimeSpan.FromMinutes(30)); return Ok(token); } Role Based Authorization Because we have configured our JWTs to include the role information, this means we can safely use the Authorize attributes across our controllers and actions:\n[HttpGet] [Route(\u0026#34;one\u0026#34;)] [Authorize(Roles = \u0026#34;User\u0026#34;)] public ActionResult\u0026lt;string\u0026gt; GetRandomMoon() { return Moons[Random.Next(Moons.Count)]; } [HttpGet] [Route(\u0026#34;two\u0026#34;)] [Authorize(Roles = \u0026#34;Admin\u0026#34;)] public ActionResult\u0026lt;string\u0026gt; GetTwoRandomMoons() { return $\u0026#34;{Moons[Random.Next(Moons.Count)]}, {Moons[Random.Next(Moons.Count)]}\u0026#34;; } Accessing The Current User Information Because we configured our JWTs to include the current username, this means we can grab the information of the user who the token belongs to - which could be helpful when making SQL etc.\n[HttpGet] [Route(\u0026#34;user\u0026#34;)] [Authorize(Roles = \u0026#34;Admin, User\u0026#34;)] public async Task\u0026lt;ActionResult\u0026lt;IdentityUser\u0026gt;\u0026gt; GetLoggedInUser() { var username = User.FindFirst(ClaimTypes.NameIdentifier)?.Value; var user = await _userManager.FindByNameAsync(username); return new OkObjectResult(user); } Adding JWT support to Swagger And finally, if you\u0026rsquo;d like to support the Authorize window in Swagger (adds the ability to pass the Bearer token with each subsequent request), add the following to your startup class:\n// Learn more about configuring Swagger/OpenAPI at https://aka.ms/aspnetcore/swashbuckle builder.Services.AddEndpointsApiExplorer(); builder.Services.AddSwaggerGen(options =\u0026gt; { var jwtSecurityScheme = new OpenApiSecurityScheme { Name = \u0026#34;Authorization\u0026#34;, Type = SecuritySchemeType.Http, Scheme = JwtBearerDefaults.AuthenticationScheme, BearerFormat = \u0026#34;JWT\u0026#34;, In = ParameterLocation.Header, Reference = new OpenApiReference { Type = ReferenceType.SecurityScheme, Id = JwtBearerDefaults.AuthenticationScheme } }; options.AddSecurityDefinition(JwtBearerDefaults.AuthenticationScheme, jwtSecurityScheme); options.AddSecurityRequirement(new OpenApiSecurityRequirement(){{ jwtSecurityScheme, new string[] {} }}); }); Testing The JWTs Using Swagger Load up swagger, login using your username and password, and grab the JWT from the response:\nNow scroll to the top of the page, and click on the Authorize button and paste in your token:\nMake sure to press Authorize to save the token!\nAnd finally, scroll to one of the endpoints that requires authentication and test it out. If you have a valid token and the correct roles - you will see the content returned. If not you\u0026rsquo;ll get a contextual http response (401 etc):\nReferences  https://docs.microsoft.com/en-us/aspnet/core/security/authorization/roles?view=aspnetcore-6.0 https://codewithmukesh.com/blog/aspnet-core-api-with-jwt-authentication/ https://weblog.west-wind.com/posts/2021/Mar/09/Role-based-JWT-Tokens-in-ASPNET-Core https://www.c-sharpcorner.com/article/how-to-add-jwt-bearer-token-authorization-functionality-in-swagger/ https://www.freecodespot.com/blog/use-jwt-bearer-authorization-in-swagger/  ","permalink":"https://dombarter.co.uk/posts/add-dotnet-jwts-to-web-api/","summary":"Authenticate and authorize users calling your .NET 6 Web API by using JSON Web Tokens (JWTs)","title":"(Part 3) Add JWT based authentication and authorization to a .NET 6 Web API"},{"content":"GitHub Repository All the code in this series can be found at https://github.com/dombarter/Solar.API\nMotivation In my previous post, I explored how to add .NET Identity to a .NET 6 Web API project. We managed to register users and then login as them. Now it would be good to add roles to those users - so once we have a token/session system setup we can restrict users to different endpoints.\nAmend The Startup The first thing we need to do is tell Identity we are going to be using roles. Go to your startup class, Program.cs:\nbuilder.Services.AddIdentity\u0026lt;IdentityUser, IdentityRole\u0026gt;(options =\u0026gt; { options.Password.RequireDigit = true; options.Password.RequireLowercase = true; options.Password.RequireNonAlphanumeric = true; options.Password.RequiredLength = 8; }) .AddRoles\u0026lt;IdentityRole\u0026gt;() // \u0026lt;--- add this line .AddEntityFrameworkStores\u0026lt;SolarDbContext\u0026gt;(); Define The Roles Next, it is a good idea to create a class where we define the possible roles in the system rather than relying on \u0026lsquo;magic strings\u0026rsquo;. Create this class, Roles.cs, somewhere:\nnamespace Solar.Common.Roles { public static class Roles { public const string Admin = \u0026#34;Admin\u0026#34;; public const string User = \u0026#34;User\u0026#34;; } } Store The Roles Now we need to make sure these roles are added to the database, so we can link them to our current user. The best place to to do this is in our startup class, Program.cs. I\u0026rsquo;ve also added a line that will make sure our database os migrated each time we startup - this way we will never miss out on a change:\n... var app = builder.Build(); using (var scope = app.Services.CreateScope()) { var services = scope.ServiceProvider; // Migrate the database  var db = services.GetRequiredService\u0026lt;SolarDbContext\u0026gt;(); db.Database.Migrate(); // Add the roles  var roleManager = services.GetRequiredService\u0026lt;RoleManager\u0026lt;IdentityRole\u0026gt;\u0026gt;(); if (!await roleManager.RoleExistsAsync(Roles.Admin)) { await roleManager.CreateAsync(new IdentityRole(Roles.Admin)); } if (!await roleManager.RoleExistsAsync(Roles.User)) { await roleManager.CreateAsync(new IdentityRole(Roles.User)); } } Assign Roles We can now assign roles to a user like so (register action):\n[HttpPost] [Route(\u0026#34;register\u0026#34;)] [AllowAnonymous] public async Task\u0026lt;IActionResult\u0026gt; Register([FromBody] RegisterDto model) { var user = new IdentityUser { UserName = model.Email, Email = model.Email }; var createResult = await _userManager.CreateAsync(user, model.Password); if (!createResult.Succeeded) { return new BadRequestObjectResult(createResult.Errors); } // Assign User role  var assignRoleResult = await _userManager.AddToRoleAsync(user, Roles.User); if (!assignRoleResult.Succeeded) { return new BadRequestObjectResult(assignRoleResult.Errors); } return Ok(); } Authorization Attributes Currently roles will have no effect until we setup a session/token system (watch out for my next post!), however once this is setup, we will be able to apply the Authorize attributes at the controller and action level like so:\n[ApiController] [Route(\u0026#34;moons\u0026#34;)] [Authorize] public class MoonController : Controller { private readonly List\u0026lt;string\u0026gt; Moons = new List\u0026lt;string\u0026gt; { \u0026#34;Moon\u0026#34;, \u0026#34;Europa\u0026#34;, \u0026#34;Titan\u0026#34;, \u0026#34;Ganymede\u0026#34;, \u0026#34;Milmas\u0026#34;, \u0026#34;Hyperion\u0026#34;, \u0026#34;Dione\u0026#34;, \u0026#34;Kiviuq\u0026#34; }; private readonly Random Random = new Random(); [HttpGet] [Route(\u0026#34;one\u0026#34;)] [Authorize(Roles = \u0026#34;User\u0026#34;)] public ActionResult\u0026lt;string\u0026gt; GetRandomMoon() { return Moons[Random.Next(Moons.Count)]; } [HttpGet] [Route(\u0026#34;two\u0026#34;)] [Authorize(Roles = \u0026#34;Admin\u0026#34;)] public ActionResult\u0026lt;string\u0026gt; GetTwoRandomMoons() { return $\u0026#34;{Moons[Random.Next(Moons.Count)]}, {Moons[Random.Next(Moons.Count)]}\u0026#34;; } } References  https://docs.microsoft.com/en-us/aspnet/core/security/authorization/roles?view=aspnetcore-6.0 https://www.c-sharpcorner.com/article/jwt-authentication-and-authorization-in-net-6-0-with-identity-framework/ https://docs.microsoft.com/en-us/aspnet/core/security/authorization/secure-data?view=aspnetcore-6.0  ","permalink":"https://dombarter.co.uk/posts/add-dotnet-identity-roles-to-web-api/","summary":"Manage your user\u0026rsquo;s roles on a .NET Web API using .NET Identity","title":"(Part 2) Add .NET Identity roles to a .NET 6 Web API"},{"content":"GitHub Repository All the code in this series can be found at https://github.com/dombarter/Solar.API\nThe Motivation There are a lot of tutorials out there for how to scaffold .NET Identity into your project, but that comes along with full register/login Razor pages. The idea of this post is to take the core .NET Identity logic to setup the database tables and provide key wrappers - but manage login and registering using our own API endpoints.\nNuGet Packages You need to start by adding the following NuGet packages to your project:\n Microsoft.AspNetCore.Identity.EntityFrameworkCore Microsoft.EntityFrameworkCore Microsoft.EntityFrameworkCore.Design Microsoft.EntityFrameworkCore.SqlServer Microsoft.EntityFrameworkCore.Tools  Database Connection You now need to get the connection to your SQL server. Add it into your appsettings.json file like so:\n{ \u0026#34;ConnectionStrings\u0026#34;: { \u0026#34;Default\u0026#34;: \u0026#34;Data Source=localhost\\\\SQLEXPRESS....\u0026#34; } } DbContext Class Instead of making a standard DbContext class we need to extend the IdentityDbContext class which will provide us with a number of extra DbSets including user and role information. Here is a basic example:\nusing Microsoft.AspNetCore.Identity; using Microsoft.AspNetCore.Identity.EntityFrameworkCore; using Microsoft.EntityFrameworkCore; namespace Solar.Data { public class SolarDbContext : IdentityDbContext\u0026lt;IdentityUser\u0026gt; { public SolarDbContext(DbContextOptions options) : base (options) { } } } Configure Entity Framework \u0026amp; Identity Now we need to configure Entity Framework, so it can connect the database and the DbContext, as well as configure Identity with key options such as password requirements.\nAdd the following lines to your startup class, Program.cs:\n// Add the database connection builder.Services.AddDbContext\u0026lt;SolarDbContext\u0026gt;(options =\u0026gt; options.UseSqlServer(builder.Configuration.GetConnectionString(\u0026#34;Default\u0026#34;))); // Setup identity builder.Services.AddIdentity\u0026lt;IdentityUser, IdentityRole\u0026gt;(options =\u0026gt; { options.Password.RequireDigit = true; options.Password.RequireLowercase = true; options.Password.RequireNonAlphanumeric = true; options.Password.RequiredLength = 8; }).AddEntityFrameworkStores\u0026lt;SolarDbContext\u0026gt;(); Database Migration With everything setup, we can now add an initial migration and apply this migration to our database:\nadd-migration init update-database You will now notice 5 or 6 tables have been created to store all the user and role information etc.\nUsing .NET Identity We can now login and register using the UserManager and SignInManager like below:\nusing Microsoft.AspNetCore.Identity; using Microsoft.AspNetCore.Mvc; using Solar.DTOs.Inbound; namespace Solar.API.Controllers { [ApiController] [Route(\u0026#34;user\u0026#34;)] public class AccountController : Controller { private readonly UserManager\u0026lt;IdentityUser\u0026gt; _userManager; private readonly SignInManager\u0026lt;IdentityUser\u0026gt; _signInManager; public AccountController(UserManager\u0026lt;IdentityUser\u0026gt; userManager, SignInManager\u0026lt;IdentityUser\u0026gt; signInManager) { _userManager = userManager; _signInManager = signInManager; } [HttpPost] [Route(\u0026#34;register\u0026#34;)] public async Task\u0026lt;IActionResult\u0026gt; Register([FromBody] RegisterDto model) { var user = new IdentityUser { UserName = model.Email, Email = model.Email }; // Create the user  var result = await _userManager.CreateAsync(user, model.Password); if (result.Succeeded) { return Ok(); } return new BadRequestObjectResult(result.Errors); } [HttpPost] [Route(\u0026#34;login\u0026#34;)] public async Task\u0026lt;IActionResult\u0026gt; Login([FromBody] LoginDto model) { // Login  var result = await _signInManager.PasswordSignInAsync(model.Email, model.Password, model.RememberMe, false); if (result.Succeeded) { return Ok(); } return BadRequest(\u0026#34;Incorrect email or password\u0026#34;); } } } And that\u0026rsquo;s it - you can now create users, and login, all using .NET Identity and managing all the DTOs and endpoint logic yourself!\nReferences  https://thecodeblogger.com/2020/01/23/adding-asp-net-core-identity-to-web-api-project/ https://www.freecodespot.com/blog/asp-net-core-identity/  ","permalink":"https://dombarter.co.uk/posts/add-dotnet-identity-to-web-api/","summary":"Manage the login and registration of your users on a .NET Web API using .NET Identity","title":"(Part 1) Add .NET Identity login and registration to a .NET 6 Web API"},{"content":"The Motivation Currently when you make an Azure Functions project you have a local.settings.json file where you setup environment rules to do with the runtime frameworks, CORs settings etc - you can also configure \u0026lsquo;application settings\u0026rsquo; that will be exposed as environment variables during runtime.\nThere are three things I would like to change about this:\n I would like to separate the concerns by keeping environment configuration in one place, and my application/logic specific settings elsewhere. It is not best practice to distribute sensitive settings by committing them into the repository - we need a different way of doing this. Currently you would have to commit the local.settings.json to distribute the sensitive values. Once local.settings.json contains no sensitive values I\u0026rsquo;d like to remove it from the .gitignore so that each member of the team will have the same local configuration.  Setup Enable User Secrets  User secrets are a tool built into .NET to allow developers to store secret information outside of the project root. It makes it less likely that secrets are accidentally committed to source control.\n Right click on the project in Visual Studio and select Manage User Secrets - this will add any required NuGet packages and alter the project file where necessary.\nOnce you have followed the required steps you should be able to click on Manage User Secrets again and an empty secrets.json file will open. This indicates that user secrets has been correctly setup.\nAdd appsettings.json file In the root of your project create an appsettings.json file and setup the insensitive values you want to store. Here is an example.\n// appsettings.json { \u0026#34;ConnectionStrings\u0026#34;: { \u0026#34;MyConnectionString\u0026#34;: \u0026#34;\u0026#34; // This value will be stored in user secrets - hence empty // (but we put it in here to remind us there is actually a value somewhere!) }, \u0026#34;General\u0026#34;: { \u0026#34;RandomColour\u0026#34;: \u0026#34;blue\u0026#34;, \u0026#34;Shape\u0026#34;: \u0026#34;triangle\u0026#34; } } You also need to make sure that the appsettings.json file is set to copy to your build output:\n\u0026lt;None Update=\u0026#34;appsettings.json\u0026#34;\u0026gt; \u0026lt;CopyToOutputDirectory\u0026gt;PreserveNewest\u0026lt;/CopyToOutputDirectory\u0026gt; \u0026lt;/None\u0026gt; Add sensitive settings to User Secrets You can see above that we store insensitive values directly in appsettings.json, but we will merge together our user secrets and these values to create our final configuration that the application has access to.\nOpen your secrets.json file and put your sensitive settings inside. E.g:\n// secrets.json { \u0026#34;ConnectionStrings\u0026#34;: { \u0026#34;MyConnectionString\u0026#34;: \u0026#34;my-sensitive-connection-string-1234-abcd\u0026#34; } // Note we didn\u0026#39;t need the General section as this is taken from appsettings.json } Create options classes To access the settings during runtime we will use dependency injection of different Options classes, we need to make these classes.\nFor our examples above we will create two classes, one for each of the sections; ConnectionStrings and General:\nConfiguration/ ConnectionStrings.cs General.cs // ConnectionStrings.cs  namespace MyProject.Configuration { public class ConnectionStrings { public string RandomColour { get; set; } public string Shape { get; set; } } } // General.cs  namespace MyProject.Configuration { public class General { public string MyConnectionString { get; set; } } } Install required packages for Dependency Injection We need to install some NuGet packages to make sure that we will be able to inject our Options classes into our functions:\n Microsoft.Azure.Functions.Extensions Microsoft.NET.Sdk.Functions ( \u0026gt;= 1.0.28 ) Microsoft.Extensions.DependencyInjections ( \u0026lt;= 3.x )  Create functions setup class to configure everything We will now create a startup class that will read values from our appsettings.json, our user secrets (and environment variables), merge them together into sections and then load into our preconfigured options objects. It will then finally register these options objects so our functions can request them via dependency injection.\nThe class should be called Startup.cs and be at the root of your project. Here is an example:\n// Startup.cs  using System; using System.IO; using MyProject.Configuration; using Microsoft.Azure.Functions.Extensions.DependencyInjection; using Microsoft.Extensions.Configuration; using Microsoft.Extensions.DependencyInjection; [assembly: FunctionsStartup(typeof(MyProject.Startup))] namespace MyProject { public class Startup : FunctionsStartup { private bool IsDevelopment =\u0026gt; string.Equals(Environment.GetEnvironmentVariable(\u0026#34;AZURE_FUNCTIONS_ENVIRONMENT\u0026#34;), \u0026#34;Development\u0026#34;, StringComparison.OrdinalIgnoreCase); /// \u0026lt;summary\u0026gt;  /// Loads in settings from various sources including environment / user secrets / appsettings  /// and binds them to various options objects  /// \u0026lt;/summary\u0026gt;  public override void Configure(IFunctionsHostBuilder builder) { // Bind connection strings  builder.Services.AddOptions\u0026lt;ConnectionStrings\u0026gt;().Configure\u0026lt;IConfiguration\u0026gt;((settings, configuration) =\u0026gt; { configuration.GetSection(nameof(ConnectionStrings)).Bind(settings); }); // Bind general settings  builder.Services.AddOptions\u0026lt;General\u0026gt;().Configure\u0026lt;IConfiguration\u0026gt;((settings, configuration) =\u0026gt; { configuration.GetSection(nameof(General)).Bind(settings); }); } /// \u0026lt;summary\u0026gt;  /// Defines the sources in which to load application settings from so they can be used above  /// \u0026lt;/summary\u0026gt;  public override void ConfigureAppConfiguration(IFunctionsConfigurationBuilder builder) { FunctionsHostBuilderContext context = builder.GetContext(); builder.ConfigurationBuilder .AddJsonFile(Path.Combine(context.ApplicationRootPath, \u0026#34;appsettings.json\u0026#34;), optional: true); if (IsDevelopment) { builder.ConfigurationBuilder.AddUserSecrets\u0026lt;Startup\u0026gt;(); } else { builder.ConfigurationBuilder.AddEnvironmentVariables(); } } } } Accessing the settings during runtime You now need to request the options objects via dependency injection in your function. Here is an example:\n// MyFunction.cs  ... private readonly IOptions\u0026lt;ConnectionStrings\u0026gt; _connectionStrings; private readonly IOptions\u0026lt;General\u0026gt; _settings; public MyFunction(IOptions\u0026lt;ConnectionStrings\u0026gt; connectionStrings, IOptions\u0026lt;General\u0026gt; settings) { _connectionStrings = connectionStrings; _settings = settings; } ... // Access a setting var shape = _settings.Value.Shape Testing the application If you now run the functions project locally you should find values stored in either user secrets or directly in appsettings.json are accessible at runtime! ðŸŽ‰ðŸŽ‰\nDeployment User secrets are not supported when deployed, hence you need to move your sensitive values to environment variables when deployed. The values hardcoded into appsettings.json will still be read. You may have noticed this in Startup.cs that we merge from different locations depending on the environment.\nThere is just once small factor you need to be made aware of. You have have to flatten your JSON when naming your environment variables. For example the following setting in our user secrets:\n{ \u0026#34;ConnectionStrings\u0026#34;: { \u0026#34;MyConnectionString\u0026#34;: \u0026#34;my-sensitive-connection-string-1234-abcd\u0026#34; } } becomes the following environment variable (using double underscore__ to indicate nesting):\nConnectionStrings__MyConnectionString = \u0026quot;my-sensitive-connection-string-1234-abcd\u0026quot; Documentation When a new member of your team comes to work on your project they will not have their user secrets setup correctly so their project will not run as expected.\nWe suggest adding a section to your README that indicates what the layout of their secrets.json should be, and where they could get the value from.\n// Please setup your secrets.json as follows, // the connection string can be found in the company password vault. { \u0026#34;ConnectionStrings\u0026#34;: { \u0026#34;MyConnectionString\u0026#34;: \u0026#34;\u0026lt;IN_PASSWORD_VAULT\u0026gt;\u0026#34; } } References  https://docs.microsoft.com/en-us/azure/azure-functions/functions-dotnet-dependency-injection https://docs.microsoft.com/en-us/aspnet/core/security/app-secrets?view=aspnetcore-6.0\u0026amp;tabs=windows  ","permalink":"https://dombarter.co.uk/posts/azure-functions-app-settings/","summary":"How to separate the concerns of environment configuration and application configuration in an azure functions project and distribute sensitive settings via user secrets and environment variables.","title":"Adding app settings and user secrets to an Azure Functions project"},{"content":"What Is Hexos? Hexos is the name of my self-employed business, through which I develop web applications, mobile applications as well as provide general IT solutions.\nWhat Is \u0026lsquo;Heatmaps By Hexos\u0026rsquo;? Heatmaps by Hexos is one of the web/cloud applications I developed and ran between 2019 and 2022. It was a successful business that sold heatmap images to schools and colleges across the UK.\n Hexos has developed a service that can create heatmaps / travel time maps to visualise public transport and driving commute times travelling towards or from a central point within a given radius. Heatmaps can be generated for any time of day, any day of the week and any location in the United Kingdom.\n The Tech Stack The tech stack was as follows:\n Express web application - Node.js AWS Lambda micro-services - Node.js AWS SQS (Simple Queue Service) AWS DynamoDB (NoSQL Object Database) Bootstrap Front-End Leaflet.js Map Provider Google Maps Directions API  An Example Of The End Product The main product that Heatmaps By Hexos generated was a selection of high definition images (also viewable as a interactive online version) - showing the travel times for the given configuration (postcode, day of the week, time of the day, transit type etc).\nHere is an example:\nA Guide Around The Web Application I\u0026rsquo;ll try to show you what the web application looked like, and how customers went through the process of requesting a heatmap from us.\nLogin Page Customers could login and sign up via our secure login and register pages. We also had forgot password functionality that would send out a reset password link with a 24 hour expiry.\nDashboard On the dashboard, the user had access to all the heatmaps they had previously or actively requested. Each heatmap was described by the postcode it represented. You could track the status of a heatmap if it was currently in the process of being generated, and once generated, view the interactive heatmap or download the images.\nRequest Heatmap On this page, the customer used pre-purchased credits to request a heatmap, by defining multiple settings:\nThe size of the heatmap defined the maximum search radius - that is, how far the algorithm would search for possible routes. The postcode represented the centre of the heatmap, either where all journeys started or finished. The map would update to place a pin at the postcode they had picked. The journeys could either be calculated by using routes taken by car, or routes taken public transport.\nThey then had to decide whether the heatmap was inbound or outbound. Inbound heatmaps would consider the latest time required to leave at a location to arrive at the centre by a given time. Outbound heatmaps would consider the earliest time you could arrive at a point, given you left the centre at a given time. The arrival or departure time would need to be set dependant on if they had picked an inbound or outbound heatmap. The day of the week then had to be picked, so that the algorithm could look at relevant routes. For example traffic may be worse in the week, or public transport not as frequent on a weekend. The user then had to submit their request - at which point the heatmap would begin processing on the AWS cloud system. When the user requested a heatmap they would receive an email: And another email when the heatmap was ready. Interactive Heatmap Viewer The interactive heatmap viewer was very similar to the downloadable images, except it let you zoom in much further, click on any cell to get specific route information, and change the maximum time filter.\nAccount Settings There was also an account settings page where you could view all the information we stored, as well as change useful settings such as turning email updates off.\nPurchase Credits Customers had to buy credits through the online dashboard that would then be credited to their account. They could then use these credits against eligible heatmap requests.\nFor example a city heatmap would cost 2 credits.\nPayments were processed through Stripe.\nHeatmap Download The user could then download a zip folder of images, in 4 definitions ranging from 512x512 to 3072x3072. The images were then broken up into 5 maximum time filters, from 1 to 3 hours.\nWe also supplied the user with a GeoJSON file with all the heatmap data, so they could make their own images.\nWhat Has Happened To \u0026lsquo;Heatmaps By Hexos\u0026rsquo; Now? Heatmaps by Hexos was a successful business that sold heatmap images to schools and colleges across the UK. They used these images to find new recruitment areas for students to come from that they hadn\u0026rsquo;t previously considered.\nIn 2022 I made the tough decision to shut this service down in order to focus on my professional development as a software engineer in industry. Other factors included the increasing time required to maintain the online service, especially with the speed at which frameworks such as Node.js are moving.\nI learnt invaluable lessons whilst building this product as well as building my network of connections, and I am sure that this is not the last we will see of Hexos!\nOther Resources \u0026amp; Information These items are considered as archived - but give some extra details about specific parts of Heatmaps by Hexos, and therefore may be of interest. These items were mainly used as marketing material.\nSheffield Digital Interview \nHeatmap Handbook The Heatmap Handbook\nService Overview Service Overview\nExample Heatmap Download heatmap.zip\n","permalink":"https://dombarter.co.uk/projects/hexos/","summary":"Running between 2019 and 2022, Hexos was a UK provider of realistic driving and public transport based heatmaps/travel-time maps for data visualisation.","title":"Heatmaps By Hexos"},{"content":"What does x.y.z stand for? In a given version number, x.y.z stands for major.minor.patch.\nE.g 1.2.4 means:\n Major release: 1 Minor release: 2 Patch release: 4  Major Versions Major versions should be incremented when you introduce huge change to a project, and specifically when you are making something no longer backwards compatible.\nMinor Versions Minor versions should be incremented when you introduce a new feature or a small to medium change. The product remains mostly the same but you have introduced something new.\nPatch Versions Patch versions should be incremented when you fix a bug, or tweak a small setting. A very small change.\nIncrementing Version Numbers When incrementing versions, major takes priority over minor and minor takes priority over patch. This means when you increment a minor version, the patch version gets reset to 0, and when you increment a major version, the minor version is reset to 0. For example:\n 1.0.0 1.0.1 1.1.0 1.2.0 2.0.0  ","permalink":"https://dombarter.co.uk/posts/semantic-versioning/","summary":"How to correctly version things using the x.y.z format","title":"Semantic Versioning"},{"content":"When did I learn about GitFlow? I learnt about GitFlow during my placement at 3squared, as it was the agreed method for managing branches across the different products and teams. As placement students we had to fully understand how GitFlow worked before we could start working on any of the companies products.\nWhy is GitFlow helpful? GitFLow helps large teams manage new features, deployments, hotfixes and ensuring there is a chain-of-command in respect of code moving between branches.\nHow does GitFlow work? Image from Medium\nThe branches  master: This branch mirrors the state of the deployed environment. develop: This branch contains a collection of all the most up to date features. feature/*: These branches are based off develop and are where the new changes are written, before being merged back into develop. release/*: These branches are based off develop and are where you make final config changes ready to be deployed to a specific environment prior to being merged into master. hotfix/*: These branches are based off master to make quick fixes that can be quickly be merged back into master to be deployed.  Starting a new feature You\u0026rsquo;ve just been assigned a ticket and it\u0026rsquo;s time to start coding! You will want to create a new feature branch based off develop. For example feature/add-profile-page. Put all your commits and changes related to a given ticket/workload on this branch. Once you have finished your changes open a pull request from your feature branch to develop.\nWhen doing feature work you will simply by winding in and out of develop.\nCode reviewing Whenever work wants to be moved onto the develop branch it must be peer reviewed by a senior developer. As a junior developer this is great because you can receive feedback but it also ensures code style is maintained and no glaring issues are introduced. It also means code can enter the the develop branch without people knowing about it!\nDeployments There is now a collection of tickets on the develop branch ready to be deployed. At this point a new release branch will be made such as release/001.002.000 based off develop. On this branch you can make final config changes such as changing database connection strings to live ones rather than local ones.\nOnce this is ready you should merge the release branch into master and perform your deployment.\nHotfixes If there are any issues on production environment and you need a quicker fix than following the \u0026lsquo;feature -\u0026gt; develop -\u0026gt; release -\u0026gt; master\u0026rsquo; route, you can make a hotfix branch. Eg. hotfix/001.002.001. In this branch you can make brief changes and quickly get them deployed.\nThis hotfix branch should be merged into master to be deployed, and back into develop to ensure your changes don\u0026rsquo;t get lost.\nMore information You can read more about GitFlow in this Atlassian guide\n","permalink":"https://dombarter.co.uk/posts/introduction-to-git-flow/","summary":"Introducing GitFlow, a well established method of branching in a git repository with multiple members","title":"Introduction to Git Flow"},{"content":" Hi, I\u0026rsquo;m Dom! I\u0026rsquo;m a Junior Software Engineer studying Computer Science at the University of Sheffield whilst on placement at 3squared. I have a wide range of experience across different platforms, languages and frameworks but I am currently specialising in C#, .NET and Vue.js development.\n Experience  3squared (2021-present) - Junior Software Engineer  3Squared supply expert consultancy \u0026amp; technology solutions developed to tackle known and emerging rail challenges. 3squared.com\n Tech Stack  C#, .NET MVC, .NET WebForms Vue.js Single Page Applications Microsoft SQL Server Microsoft Azure Azure Functions Azure Static Web Apps Azure App Service Azure DevOps  Projects Throughout my time at 3squared I worked on a number of different projects including:\n RailSmart Docs, Forms, Hub. A digital solution built in Vue.js, .NET and WebForms for the management of versioned documents, as well as the control and distribution of forms and form responses. I have helped fix many bugs across the whole application, as well as implement completely new features such as new pages, new components and even implementing Application Insights to gain better logging. I worked throughout the full stack. Forge UI Framework. A reusable component library written in Vue.js that is shared across the different apps in the companies SaaS suite. I helped write new components and build a fifth version that supported tree-shaking. It was published to an Azure Artifacts registry. Forge CDN Explorer. A Vue.js single page application that displays all the contents of our company CDN and allows users to browse the CDN and find images/assets they need for their application with live thumbnails and previews. They can then directly copy production URLs to various assets. I wrote this application as well as setup the full automated deployment to an Azure static web app.  Daily Responsibilities  Attend daily stand-ups Attend sprint retrospectives to reflect on our ability as a team Attend backlog refinement sessions to continue to prioritise work Attend estimation sessions to gain insight into how long work will take to complete Fix bugs that are raised by clients and the QA team Implement new features and change requests against my own estimates and set out requirements Research and implement new technologies Make deployments to different tiers of environments (dev, stag, prod) Work with a range of people in my team including other Devs, QA, Business Analysts, Project Managers, Design/UX Team and IT/DevOps.   Elements Technology (2019-2021) - Junior Full Stack Developer  The simple way to make your factory smart. We make it easy for staff across your business to access live and accurate information about your manufacturing processes, wherever they are. elementstechnology.co.uk/\n Tech Stack Projects  Education ","permalink":"https://dombarter.co.uk/about/","summary":"Hi, I\u0026rsquo;m Dom! I\u0026rsquo;m a Junior Software Engineer studying Computer Science at the University of Sheffield whilst on placement at 3squared. I have a wide range of experience across different platforms, languages and frameworks but I am currently specialising in C#, .NET and Vue.js development.\n Experience  3squared (2021-present) - Junior Software Engineer  3Squared supply expert consultancy \u0026amp; technology solutions developed to tackle known and emerging rail challenges.","title":"About"},{"content":"Ever wondered what happens when you fill a computer screen with llamas?\nNo I don\u0026rsquo;t think many people have!\nWell it turns out I wanted to find out\u0026hellip;\nBuilt in plain HTML, CSS and JavaScript with help from Phaser.js - please go and enjoy my llama physics engine:\ndombarter.co.uk/llama/\nAnd if you\u0026rsquo;re interested in the source code:\nView GitHub repository\n","permalink":"https://dombarter.co.uk/projects/llamas/","summary":"It\u0026rsquo;s llamas and physics. Who knows what could happen?","title":"Llama Physics Engine"}]